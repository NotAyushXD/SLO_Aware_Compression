{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ccaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_baseline_evaluation.py\n",
    "\"\"\"\n",
    "End-to-end baseline evaluation orchestration\n",
    "Runs complete pipeline: preprocessing → server → load tests → evaluation\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from preprocessing import DataPreprocessor\n",
    "from server import SingleVariantServer\n",
    "from load_generator import ClosedLoopLoadGenerator\n",
    "from metrics import MetricsCalculator\n",
    "from evaluation import HeldOutEvaluator\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_data(data_dir: str = \"data/processed\"):\n",
    "    \"\"\"Load preprocessed datasets from JSONL files\"\"\"\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for split_name, split_list in [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]:\n",
    "        path = os.path.join(data_dir, f\"{split_name}_data.jsonl\")\n",
    "        if os.path.exists(path):\n",
    "            logger.info(f\"Loading {split_name} data from {path}\")\n",
    "            with open(path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        split_list.append(json.loads(line))\n",
    "        else:\n",
    "            logger.warning(f\"File not found: {path}\")\n",
    "    \n",
    "    logger.info(f\"Loaded data: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c110277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"Run end-to-end baseline evaluation pipeline\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"END-TO-END BASELINE EVALUATION: MED-ONLY SERVER (8-BIT QUANTIZATION)\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Data preprocessing (if needed)\n",
    "    if args.preprocess:\n",
    "        logger.info(\"\\n[STEP 0] PREPROCESSING DATASETS\")\n",
    "        logger.info(\"-\"*80)\n",
    "        \n",
    "        preprocessor = DataPreprocessor(\n",
    "            data_dir=args.data_dir,\n",
    "            output_dir=args.processed_dir\n",
    "        )\n",
    "        train, val, test = preprocessor.run_pipeline()\n",
    "    \n",
    "    # Step 2: Load preprocessed data\n",
    "    logger.info(\"\\n[STEP 1] LOADING DATA\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    train_data, val_data, test_data = load_data(args.processed_dir)\n",
    "    \n",
    "    if not val_data or not test_data:\n",
    "        logger.error(\"No validation or test data found!\")\n",
    "        return\n",
    "    \n",
    "    # Use subset for faster iteration (optional)\n",
    "    if args.data_subset > 0:\n",
    "        val_data = val_data[:args.data_subset]\n",
    "        test_data = test_data[:args.data_subset]\n",
    "        logger.info(f\"Using subset: val={len(val_data)}, test={len(test_data)}\")\n",
    "    \n",
    "    # Step 3: Initialize server\n",
    "    logger.info(\"\\n[STEP 2] INITIALIZING SERVER\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        server = SingleVariantServer(\n",
    "            model_name=args.model_name,\n",
    "            variant=\"med\",\n",
    "            device=args.device,\n",
    "            dtype=args.dtype\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize server: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Run load tests at multiple concurrency levels\n",
    "    logger.info(\"\\n[STEP 3] RUNNING LOAD TESTS\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    load_test_results = {}\n",
    "    all_metrics_summary = []\n",
    "    \n",
    "    for concurrency in args.concurrencies:\n",
    "        logger.info(f\"\\n>>> Testing with concurrency={concurrency}\")\n",
    "        \n",
    "        # Create load generator\n",
    "        load_gen = ClosedLoopLoadGenerator(\n",
    "            inference_func=server.generate,\n",
    "            max_concurrency=concurrency,\n",
    "            num_requests=args.num_requests,\n",
    "            data_loader=val_data\n",
    "        )\n",
    "        \n",
    "        # Run load test\n",
    "        start_time = time.time()\n",
    "        metrics = load_gen.run()\n",
    "        load_duration = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        calc = MetricsCalculator(metrics)\n",
    "        test_metrics = calc.compute_all_metrics()\n",
    "        load_test_results[concurrency] = test_metrics\n",
    "        \n",
    "        # Print report\n",
    "        calc.print_report(\n",
    "            title=f\"LOAD TEST RESULTS (Concurrency {concurrency})\"\n",
    "        )\n",
    "        \n",
    "        # Save detailed metrics to JSON\n",
    "        metrics_file = os.path.join(\n",
    "            args.output_dir,\n",
    "            f\"metrics_concurrency_{concurrency}.json\"\n",
    "        )\n",
    "        calc.save_metrics(metrics_file)\n",
    "        \n",
    "        # Save individual request logs\n",
    "        requests_file = os.path.join(\n",
    "            args.output_dir,\n",
    "            f\"requests_concurrency_{concurrency}.jsonl\"\n",
    "        )\n",
    "        load_gen.save_metrics(requests_file)\n",
    "        \n",
    "        # Summary entry\n",
    "        summary_entry = {\n",
    "            \"concurrency\": concurrency,\n",
    "            \"num_requests\": args.num_requests,\n",
    "            \"duration_sec\": load_duration,\n",
    "            \"success_rate\": test_metrics[\"summary\"][\"success_rate\"],\n",
    "            \"throughput_tokens_per_sec\": test_metrics[\"summary\"][\"throughput_tokens_per_sec\"],\n",
    "            \"ttft_p99_ms\": test_metrics[\"ttft\"][\"p99\"],\n",
    "            \"tpot_p95_ms\": test_metrics[\"tpot\"][\"p95\"],\n",
    "            \"e2e_p99_ms\": test_metrics[\"e2e_latency\"][\"p99\"],\n",
    "            \"slo_compliance\": test_metrics[\"summary\"][\"slo_compliance\"],\n",
    "            \"slo_violations\": test_metrics[\"summary\"][\"slo_violations\"]\n",
    "        }\n",
    "        all_metrics_summary.append(summary_entry)\n",
    "    \n",
    "    # Step 5: Evaluate accuracy on held-out test set\n",
    "    logger.info(\"\\n[STEP 4] EVALUATING ACCURACY\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        evaluator = HeldOutEvaluator(\n",
    "            model=server,\n",
    "            data_loader=test_data,\n",
    "            batch_size=32\n",
    "        )\n",
    "        eval_results = evaluator.evaluate()\n",
    "        \n",
    "        # Save eval results\n",
    "        eval_file = os.path.join(args.output_dir, \"eval_results.json\")\n",
    "        with open(eval_file, 'w') as f:\n",
    "            json.dump(eval_results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved evaluation results to {eval_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")\n",
    "        eval_results = {}\n",
    "    \n",
    "    # Step 6: Summary report\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"FINAL SUMMARY REPORT\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nLoad Test Results by Concurrency:\")\n",
    "    print(f\"{'Concurrency':<12} {'Throughput':<18} {'TTFT P99':<12} {'TPOT P95':<12} {'E2E P99':<12} {'SLO Compl':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for summary in all_metrics_summary:\n",
    "        print(f\"{summary['concurrency']:<12} \"\n",
    "              f\"{summary['throughput_tokens_per_sec']:<18.1f} \"\n",
    "              f\"{summary['ttft_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['tpot_p95_ms']:<12.1f} \"\n",
    "              f\"{summary['e2e_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['slo_compliance']*100:<10.1f}%\")\n",
    "    \n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    if eval_results:\n",
    "        for dataset_type in sorted(eval_results.keys()):\n",
    "            if dataset_type != \"overall\":\n",
    "                result = eval_results[dataset_type]\n",
    "                print(f\"  {dataset_type.upper():<10s}: {result['em']*100:6.2f}% \"\n",
    "                      f\"({result['correct_count']}/{result['total_count']})\")\n",
    "        \n",
    "        overall = eval_results.get(\"overall\", {})\n",
    "        print(f\"  {'OVERALL':<10s}: {overall.get('em', 0)*100:6.2f}% \"\n",
    "              f\"({overall.get('correct_count', 0)}/{overall.get('total_count', 0)})\")\n",
    "    else:\n",
    "        print(\"  No evaluation results available\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = os.path.join(args.output_dir, \"summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"load_test_summary\": all_metrics_summary,\n",
    "            \"eval_results\": eval_results,\n",
    "            \"config\": {\n",
    "                \"model_name\": args.model_name,\n",
    "                \"variant\": \"med\",\n",
    "                \"num_requests\": args.num_requests,\n",
    "                \"concurrencies\": args.concurrencies,\n",
    "                \"device\": args.device\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"BASELINE EVALUATION COMPLETE\")\n",
    "    logger.info(f\"Results saved to: {args.output_dir}\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"\\nFiles generated:\")\n",
    "    logger.info(f\"  - summary.json (overall summary)\")\n",
    "    logger.info(f\"  - eval_results.json (accuracy metrics)\")\n",
    "    for concurrency in args.concurrencies:\n",
    "        logger.info(f\"  - metrics_concurrency_{concurrency}.json\")\n",
    "        logger.info(f\"  - requests_concurrency_{concurrency}.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bba44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Define your config here (edit directly in the notebook)\n",
    "config = {\n",
    "    \"output_dir\": \"./data/processed\",\n",
    "    \"data_dir\": \"./results/baseline_med\",\n",
    "    \"processed_dir\": \"./data/processed\",\n",
    "    \"preprocess\": True,\n",
    "    \"data_subset\": 0,                 # 0 = use all; >0 = use first N\n",
    "    \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"device\": \"mps\",\n",
    "    \"dtype\": \"float16\",\n",
    "    \"concurrencies\": [1,2,4,8,16,32],\n",
    "    \"num_requests\": 5000,\n",
    "}\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Run end-to-end baseline evaluation pipeline in notebook.\"\"\"\n",
    "\n",
    "    # Convenience local vars (to avoid args.)\n",
    "    output_dir     = config[\"output_dir\"]\n",
    "    data_dir       = config[\"data_dir\"]\n",
    "    processed_dir  = config[\"processed_dir\"]\n",
    "    preprocess     = config[\"preprocess\"]\n",
    "    data_subset    = config[\"data_subset\"]\n",
    "    model_name     = config[\"model_name\"]\n",
    "    device         = config[\"device\"]\n",
    "    dtype          = config[\"dtype\"]\n",
    "    concurrencies  = config[\"concurrencies\"]\n",
    "    num_requests   = config[\"num_requests\"]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"END-TO-END BASELINE EVALUATION: MED-ONLY SERVER (8-BIT QUANTIZATION)\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    # Step 1: Data preprocessing (if needed)\n",
    "    if preprocess:\n",
    "        logger.info(\"\\n[STEP 0] PREPROCESSING DATASETS\")\n",
    "        logger.info(\"-\"*80)\n",
    "\n",
    "        preprocessor = DataPreprocessor(\n",
    "            data_dir=data_dir,\n",
    "            output_dir=processed_dir\n",
    "        )\n",
    "        train, val, test = preprocessor.run_pipeline()\n",
    "\n",
    "    # Step 2: Load preprocessed data\n",
    "    logger.info(\"\\n[STEP 1] LOADING DATA\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    train_data, val_data, test_data = load_data(processed_dir)\n",
    "\n",
    "    if not val_data or not test_data:\n",
    "        logger.error(\"No validation or test data found!\")\n",
    "        return\n",
    "\n",
    "    # Use subset for faster iteration (optional)\n",
    "    if data_subset > 0:\n",
    "        val_data = val_data[:data_subset]\n",
    "        test_data = test_data[:data_subset]\n",
    "        logger.info(f\"Using subset: val={len(val_data)}, test={len(test_data)}\")\n",
    "\n",
    "    # Step 3: Initialize server\n",
    "    logger.info(\"\\n[STEP 2] INITIALIZING SERVER\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    try:\n",
    "        server = SingleVariantServer(\n",
    "            model_name=model_name,\n",
    "            variant=\"med\",\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize server: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Run load tests at multiple concurrency levels\n",
    "    logger.info(\"\\n[STEP 3] RUNNING LOAD TESTS\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    load_test_results = {}\n",
    "    all_metrics_summary = []\n",
    "\n",
    "    for concurrency in concurrencies:\n",
    "        logger.info(f\"\\n>>> Testing with concurrency={concurrency}\")\n",
    "\n",
    "        # Create load generator\n",
    "        load_gen = ClosedLoopLoadGenerator(\n",
    "            inference_func=server.generate,\n",
    "            max_concurrency=concurrency,\n",
    "            num_requests=num_requests,\n",
    "            data_loader=val_data\n",
    "        )\n",
    "\n",
    "        # Run load test\n",
    "        start_time = time.time()\n",
    "        metrics = load_gen.run()\n",
    "        load_duration = time.time() - start_time\n",
    "\n",
    "        # Calculate metrics\n",
    "        calc = MetricsCalculator(metrics)\n",
    "        test_metrics = calc.compute_all_metrics()\n",
    "        load_test_results[concurrency] = test_metrics\n",
    "\n",
    "        # Print report\n",
    "        calc.print_report(\n",
    "            title=f\"LOAD TEST RESULTS (Concurrency {concurrency})\"\n",
    "        )\n",
    "\n",
    "        # Save detailed metrics to JSON\n",
    "        metrics_file = os.path.join(\n",
    "            output_dir,\n",
    "            f\"metrics_concurrency_{concurrency}.json\"\n",
    "        )\n",
    "        calc.save_metrics(metrics_file)\n",
    "\n",
    "        # Save individual request logs\n",
    "        requests_file = os.path.join(\n",
    "            output_dir,\n",
    "            f\"requests_concurrency_{concurrency}.jsonl\"\n",
    "        )\n",
    "        load_gen.save_metrics(requests_file)\n",
    "\n",
    "        # Summary entry\n",
    "        summary_entry = {\n",
    "            \"concurrency\": concurrency,\n",
    "            \"num_requests\": num_requests,\n",
    "            \"duration_sec\": load_duration,\n",
    "            \"success_rate\": test_metrics[\"summary\"][\"success_rate\"],\n",
    "            \"throughput_tokens_per_sec\": test_metrics[\"summary\"][\"throughput_tokens_per_sec\"],\n",
    "            \"ttft_p99_ms\": test_metrics[\"ttft\"][\"p99\"],\n",
    "            \"tpot_p95_ms\": test_metrics[\"tpot\"][\"p95\"],\n",
    "            \"e2e_p99_ms\": test_metrics[\"e2e_latency\"][\"p99\"],\n",
    "            \"slo_compliance\": test_metrics[\"summary\"][\"slo_compliance\"],\n",
    "            \"slo_violations\": test_metrics[\"summary\"][\"slo_violations\"]\n",
    "        }\n",
    "        all_metrics_summary.append(summary_entry)\n",
    "\n",
    "    # Step 5: Evaluate accuracy on held-out test set\n",
    "    logger.info(\"\\n[STEP 4] EVALUATING ACCURACY\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    try:\n",
    "        evaluator = HeldOutEvaluator(\n",
    "            model=server,\n",
    "            data_loader=test_data,\n",
    "            batch_size=32\n",
    "        )\n",
    "        eval_results = evaluator.evaluate()\n",
    "\n",
    "        # Save eval results\n",
    "        eval_file = os.path.join(output_dir, \"eval_results.json\")\n",
    "        with open(eval_file, 'w') as f:\n",
    "            json.dump(eval_results, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Saved evaluation results to {eval_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")\n",
    "        eval_results = {}\n",
    "\n",
    "    # Step 6: Summary report\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"FINAL SUMMARY REPORT\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    print(\"\\nLoad Test Results by Concurrency:\")\n",
    "    print(f\"{'Concurrency':<12} {'Throughput':<18} {'TTFT P99':<12} {'TPOT P95':<12} {'E2E P99':<12} {'SLO Compl':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for summary in all_metrics_summary:\n",
    "        print(f\"{summary['concurrency']:<12} \"\n",
    "              f\"{summary['throughput_tokens_per_sec']:<18.1f} \"\n",
    "              f\"{summary['ttft_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['tpot_p95_ms']:<12.1f} \"\n",
    "              f\"{summary['e2e_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['slo_compliance']*100:<10.1f}%\")\n",
    "\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    if eval_results:\n",
    "        for dataset_type in sorted(eval_results.keys()):\n",
    "            if dataset_type != \"overall\":\n",
    "                result = eval_results[dataset_type]\n",
    "                print(f\"  {dataset_type.upper():<10s}: {result['em']*100:6.2f}% \"\n",
    "                      f\"({result['correct_count']}/{result['total_count']})\")\n",
    "\n",
    "        overall = eval_results.get(\"overall\", {})\n",
    "        print(f\"  {'OVERALL':<10s}: {overall.get('em', 0)*100:6.2f}% \"\n",
    "              f\"({overall.get('correct_count', 0)}/{overall.get('total_count', 0)})\")\n",
    "    else:\n",
    "        print(\"  No evaluation results available\")\n",
    "\n",
    "    # Save summary\n",
    "    summary_file = os.path.join(output_dir, \"summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"load_test_summary\": all_metrics_summary,\n",
    "            \"eval_results\": eval_results,\n",
    "            \"config\": {\n",
    "                \"model_name\": model_name,\n",
    "                \"variant\": \"med\",\n",
    "                \"num_requests\": num_requests,\n",
    "                \"concurrencies\": concurrencies,\n",
    "                \"device\": device\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"BASELINE EVALUATION COMPLETE\")\n",
    "    logger.info(f\"Results saved to: {output_dir}\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"\\nFiles generated:\")\n",
    "    logger.info(f\"  - summary.json (overall summary)\")\n",
    "    logger.info(f\"  - eval_results.json (accuracy metrics)\")\n",
    "    for concurrency in concurrencies:\n",
    "        logger.info(f\"  - metrics_concurrency_{concurrency}.json\")\n",
    "        logger.info(f\"  - requests_concurrency_{concurrency}.jsonl\")\n",
    "\n",
    "    # Optionally return things for further notebook analysis\n",
    "    return {\n",
    "        \"load_test_summary\": all_metrics_summary,\n",
    "        \"eval_results\": eval_results,\n",
    "        \"config\": config,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be8b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 00:51:37,312 - __main__ - INFO - ================================================================================\n",
      "2026-01-15 00:51:37,312 - __main__ - INFO - END-TO-END BASELINE EVALUATION: MED-ONLY SERVER (8-BIT QUANTIZATION)\n",
      "2026-01-15 00:51:37,313 - __main__ - INFO - ================================================================================\n",
      "2026-01-15 00:51:37,313 - __main__ - INFO - \n",
      "[STEP 0] PREPROCESSING DATASETS\n",
      "2026-01-15 00:51:37,313 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-15 00:51:37,389 - preprocessing - INFO - Initialized DataPreprocessor\n",
      "2026-01-15 00:51:37,389 - preprocessing - INFO -   Data dir: ./results/baseline_med\n",
      "2026-01-15 00:51:37,390 - preprocessing - INFO -   Output dir: ./data/processed\n",
      "2026-01-15 00:51:37,390 - preprocessing - INFO - ======================================================================\n",
      "2026-01-15 00:51:37,390 - preprocessing - INFO - STARTING DATA PREPROCESSING PIPELINE\n",
      "2026-01-15 00:51:37,390 - preprocessing - INFO - ======================================================================\n",
      "2026-01-15 00:51:37,391 - preprocessing - INFO - \n",
      "[STEP 1] Processing MMLU\n",
      "2026-01-15 00:51:37,391 - preprocessing - INFO - ----------------------------------------------------------------------\n",
      "2026-01-15 00:51:37,391 - preprocessing - INFO - Processing MMLU dataset...\n",
      "2026-01-15 00:51:40,973 - preprocessing - INFO -   Processing MMLU validation split: 1531 examples\n",
      "2026-01-15 00:51:41,029 - preprocessing - INFO -   Processing MMLU test split: 14042 examples\n",
      "2026-01-15 00:51:41,488 - preprocessing - INFO - Processed 15573 MMLU examples\n",
      "2026-01-15 00:51:41,539 - preprocessing - INFO - Saved MMLU to ./data/processed/mmlu_processed.jsonl\n",
      "2026-01-15 00:51:41,540 - preprocessing - INFO - \n",
      "[STEP 2] Processing GSM8K\n",
      "2026-01-15 00:51:41,540 - preprocessing - INFO - ----------------------------------------------------------------------\n",
      "2026-01-15 00:51:41,540 - preprocessing - INFO - Processing GSM8K dataset...\n",
      "2026-01-15 00:51:44,619 - preprocessing - INFO -   Processing GSM8K train split: 7473 examples\n",
      "2026-01-15 00:51:44,860 - preprocessing - INFO -   Processing GSM8K test split: 1319 examples\n",
      "2026-01-15 00:51:44,901 - preprocessing - INFO - Processed 8792 GSM8K examples\n",
      "2026-01-15 00:51:44,925 - preprocessing - INFO - Saved GSM8K to ./data/processed/gsm8k_processed.jsonl\n",
      "2026-01-15 00:51:44,926 - preprocessing - INFO - \n",
      "[STEP 3] Combining and creating splits\n",
      "2026-01-15 00:51:44,926 - preprocessing - INFO - ----------------------------------------------------------------------\n",
      "2026-01-15 00:51:44,926 - preprocessing - INFO - Combining datasets and creating splits...\n",
      "2026-01-15 00:51:44,927 - preprocessing - INFO - Loading mmlu_processed...\n",
      "2026-01-15 00:51:44,970 - preprocessing - INFO - Loading gsm8k_processed...\n",
      "2026-01-15 00:51:44,989 - preprocessing - INFO - Total examples loaded: 24365\n",
      "2026-01-15 00:51:44,991 - preprocessing - INFO -   easy  : 4093 examples\n",
      "2026-01-15 00:51:44,994 - preprocessing - INFO -   medium: 16480 examples\n",
      "2026-01-15 00:51:44,996 - preprocessing - INFO -   hard  : 3792 examples\n",
      "2026-01-15 00:51:45,050 - preprocessing - INFO - train split: 14618 examples | Easy:  2455 | Medium:  9888 | Hard:  2275\n",
      "2026-01-15 00:51:45,070 - preprocessing - INFO - val   split:  4873 examples | Easy:   819 | Medium:  3296 | Hard:   758\n",
      "2026-01-15 00:51:45,088 - preprocessing - INFO - test  split:  4874 examples | Easy:   819 | Medium:  3296 | Hard:   759\n",
      "2026-01-15 00:51:45,089 - preprocessing - INFO - \n",
      "======================================================================\n",
      "2026-01-15 00:51:45,089 - preprocessing - INFO - PREPROCESSING COMPLETE\n",
      "2026-01-15 00:51:45,089 - preprocessing - INFO - ======================================================================\n",
      "2026-01-15 00:51:45,093 - __main__ - INFO - \n",
      "[STEP 1] LOADING DATA\n",
      "2026-01-15 00:51:45,093 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-15 00:51:45,093 - __main__ - INFO - Loading train data from ./data/processed/train_data.jsonl\n",
      "2026-01-15 00:51:45,145 - __main__ - INFO - Loading val data from ./data/processed/val_data.jsonl\n",
      "2026-01-15 00:51:45,156 - __main__ - INFO - Loading test data from ./data/processed/test_data.jsonl\n",
      "2026-01-15 00:51:45,169 - __main__ - INFO - Loaded data: train=14618, val=4873, test=4874\n",
      "2026-01-15 00:51:45,170 - __main__ - INFO - \n",
      "[STEP 2] INITIALIZING SERVER\n",
      "2026-01-15 00:51:45,170 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-15 00:51:45,185 - server - INFO - Initializing MED server\n",
      "2026-01-15 00:51:45,185 - server - INFO -   Model: meta-llama/Llama-2-7b-chat-hf\n",
      "2026-01-15 00:51:45,186 - server - INFO -   Device: mps\n",
      "2026-01-15 00:51:45,186 - server - INFO -   Dtype: float16\n",
      "2026-01-15 00:51:46,386 - server - INFO - Tokenizer loaded: LlamaTokenizerFast\n",
      "2026-01-15 00:51:46,386 - server - INFO - Loading model with 8-bit quantization...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccc2087f57f4db0bcd5c158d849d0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 00:51:57,140 - server - INFO - Model loaded successfully\n",
      "2026-01-15 00:51:57,142 - server - INFO - Model size: 6.74B parameters\n",
      "2026-01-15 00:51:57,142 - __main__ - INFO - \n",
      "[STEP 3] RUNNING LOAD TESTS\n",
      "2026-01-15 00:51:57,143 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-15 00:51:57,143 - __main__ - INFO - \n",
      ">>> Testing with concurrency=1\n",
      "2026-01-15 00:51:57,144 - load_generator - INFO - Initialized ClosedLoopLoadGenerator\n",
      "2026-01-15 00:51:57,144 - load_generator - INFO -   Concurrency: 1\n",
      "2026-01-15 00:51:57,145 - load_generator - INFO -   Total requests: 5000\n",
      "2026-01-15 00:51:57,145 - load_generator - INFO -   Data pool size: 4873\n",
      "2026-01-15 00:51:57,146 - load_generator - INFO - ======================================================================\n",
      "2026-01-15 00:51:57,146 - load_generator - INFO - STARTING LOAD TEST: 5000 requests @ 1 concurrency\n",
      "2026-01-15 00:51:57,146 - load_generator - INFO - ======================================================================\n",
      "2026-01-15 00:52:06,998 - load_generator - INFO - Load test complete in 9.9s\n",
      "2026-01-15 00:52:06,998 - metrics - INFO - Initialized MetricsCalculator with 2 metrics\n",
      "2026-01-15 00:52:07,002 - metrics - INFO - Saved metrics to ./data/processed/metrics_concurrency_1.json\n",
      "2026-01-15 00:52:07,003 - load_generator - INFO - Saved metrics to ./data/processed/requests_concurrency_1.jsonl\n",
      "2026-01-15 00:52:07,003 - __main__ - INFO - \n",
      ">>> Testing with concurrency=2\n",
      "2026-01-15 00:52:07,004 - load_generator - INFO - Initialized ClosedLoopLoadGenerator\n",
      "2026-01-15 00:52:07,004 - load_generator - INFO -   Concurrency: 2\n",
      "2026-01-15 00:52:07,004 - load_generator - INFO -   Total requests: 5000\n",
      "2026-01-15 00:52:07,004 - load_generator - INFO -   Data pool size: 4873\n",
      "2026-01-15 00:52:07,004 - load_generator - INFO - ======================================================================\n",
      "2026-01-15 00:52:07,005 - load_generator - INFO - STARTING LOAD TEST: 5000 requests @ 2 concurrency\n",
      "2026-01-15 00:52:07,005 - load_generator - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOAD TEST RESULTS (Concurrency 1)\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Total Requests:             2\n",
      "  Successful:                 2\n",
      "  Failed:                     0\n",
      "  Success Rate:          100.00%\n",
      "  Total Duration:          9.85 seconds\n",
      "  Throughput:               0.2 tokens/sec\n",
      "  SLO Compliance:          0.00%\n",
      "  SLO Violations:             2\n",
      "  Escalation Rate:         0.00%\n",
      "\n",
      "TTFT (Time-to-First-Token) in milliseconds:\n",
      "  P50:  3753.86 ms\n",
      "  P75:  4892.87 ms\n",
      "  P90:  5576.28 ms\n",
      "  P95:  5804.08 ms\n",
      "  P99:  5986.33 ms\n",
      "  Mean: 3753.86 ms (±2278.03)\n",
      "\n",
      "TPOT (Time-Per-Output-Token) in milliseconds:\n",
      "  P50:  1167.85 ms\n",
      "  P75:  1220.17 ms\n",
      "  P90:  1251.56 ms\n",
      "  P95:  1262.02 ms\n",
      "  P99:  1270.39 ms\n",
      "  Mean: 1167.85 ms (±104.63)\n",
      "\n",
      "E2E Latency (End-to-End) in milliseconds:\n",
      "  P50:  4924.49 ms\n",
      "  P75:  6116.71 ms\n",
      "  P90:  6832.04 ms\n",
      "  P95:  7070.48 ms\n",
      "  P99:  7261.24 ms\n",
      "  Mean: 4924.49 ms (±2384.43)\n",
      "\n",
      "Queue Wait Time in milliseconds:\n",
      "  P50:     0.02 ms\n",
      "  P95:     0.03 ms\n",
      "  P99:     0.03 ms\n",
      "  Mean:    0.02 ms\n",
      "\n",
      "Sample SLO Violations (first 10):\n",
      "  1. Request 0 (easy)\n",
      "     TTFT: 6031.9ms > 200ms SLO\n",
      "     TPOT: 1272.5ms > 20ms SLO\n",
      "  2. Request 1 (easy)\n",
      "     TTFT: 1475.8ms > 200ms SLO\n",
      "     TPOT: 1063.2ms > 20ms SLO\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 00:52:19,426 - load_generator - INFO - Load test complete in 12.4s\n",
      "2026-01-15 00:52:19,426 - metrics - INFO - Initialized MetricsCalculator with 4 metrics\n",
      "2026-01-15 00:52:19,430 - metrics - INFO - Saved metrics to ./data/processed/metrics_concurrency_2.json\n",
      "2026-01-15 00:52:19,430 - load_generator - INFO - Saved metrics to ./data/processed/requests_concurrency_2.jsonl\n",
      "2026-01-15 00:52:19,430 - __main__ - INFO - \n",
      ">>> Testing with concurrency=4\n",
      "2026-01-15 00:52:19,431 - load_generator - INFO - Initialized ClosedLoopLoadGenerator\n",
      "2026-01-15 00:52:19,431 - load_generator - INFO -   Concurrency: 4\n",
      "2026-01-15 00:52:19,431 - load_generator - INFO -   Total requests: 5000\n",
      "2026-01-15 00:52:19,431 - load_generator - INFO -   Data pool size: 4873\n",
      "2026-01-15 00:52:19,431 - load_generator - INFO - ======================================================================\n",
      "2026-01-15 00:52:19,432 - load_generator - INFO - STARTING LOAD TEST: 5000 requests @ 4 concurrency\n",
      "2026-01-15 00:52:19,432 - load_generator - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOAD TEST RESULTS (Concurrency 2)\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Total Requests:             4\n",
      "  Successful:                 4\n",
      "  Failed:                     0\n",
      "  Success Rate:          100.00%\n",
      "  Total Duration:         12.42 seconds\n",
      "  Throughput:               0.3 tokens/sec\n",
      "  SLO Compliance:          0.00%\n",
      "  SLO Violations:             4\n",
      "  Escalation Rate:         0.00%\n",
      "\n",
      "TTFT (Time-to-First-Token) in milliseconds:\n",
      "  P50:  3233.97 ms\n",
      "  P75:  3989.69 ms\n",
      "  P90:  4286.63 ms\n",
      "  P95:  4385.60 ms\n",
      "  P99:  4464.79 ms\n",
      "  Mean: 3360.24 ms (±829.76)\n",
      "\n",
      "TPOT (Time-Per-Output-Token) in milliseconds:\n",
      "  P50:  2885.73 ms\n",
      "  P75:  3065.75 ms\n",
      "  P90:  3143.40 ms\n",
      "  P95:  3169.28 ms\n",
      "  P99:  3189.98 ms\n",
      "  Mean: 2815.01 ms (±340.60)\n",
      "\n",
      "E2E Latency (End-to-End) in milliseconds:\n",
      "  P50:  6209.28 ms\n",
      "  P75:  6808.61 ms\n",
      "  P90:  7229.42 ms\n",
      "  P95:  7369.69 ms\n",
      "  P99:  7481.90 ms\n",
      "  Mean: 6179.55 ms (±996.33)\n",
      "\n",
      "Queue Wait Time in milliseconds:\n",
      "  P50:     0.00 ms\n",
      "  P95:     0.01 ms\n",
      "  P99:     0.01 ms\n",
      "  Mean:    0.00 ms\n",
      "\n",
      "Sample SLO Violations (first 10):\n",
      "  1. Request 1 (easy)\n",
      "     TTFT: 2488.5ms > 200ms SLO\n",
      "     TPOT: 2293.4ms > 20ms SLO\n",
      "  2. Request 0 (easy)\n",
      "     TTFT: 2643.2ms > 200ms SLO\n",
      "     TPOT: 3195.2ms > 20ms SLO\n",
      "  3. Request 2 (easy)\n",
      "     TTFT: 4484.6ms > 200ms SLO\n",
      "     TPOT: 3022.6ms > 20ms SLO\n",
      "  4. Request 3 (easy)\n",
      "     TTFT: 3824.7ms > 200ms SLO\n",
      "     TPOT: 2748.8ms > 20ms SLO\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 00:52:45,596 - load_generator - INFO - Load test complete in 26.2s\n",
      "2026-01-15 00:52:45,597 - metrics - INFO - Initialized MetricsCalculator with 8 metrics\n",
      "2026-01-15 00:52:45,600 - metrics - INFO - Saved metrics to ./data/processed/metrics_concurrency_4.json\n",
      "2026-01-15 00:52:45,601 - load_generator - INFO - Saved metrics to ./data/processed/requests_concurrency_4.jsonl\n",
      "2026-01-15 00:52:45,601 - __main__ - INFO - \n",
      ">>> Testing with concurrency=8\n",
      "2026-01-15 00:52:45,601 - load_generator - INFO - Initialized ClosedLoopLoadGenerator\n",
      "2026-01-15 00:52:45,602 - load_generator - INFO -   Concurrency: 8\n",
      "2026-01-15 00:52:45,602 - load_generator - INFO -   Total requests: 5000\n",
      "2026-01-15 00:52:45,602 - load_generator - INFO -   Data pool size: 4873\n",
      "2026-01-15 00:52:45,602 - load_generator - INFO - ======================================================================\n",
      "2026-01-15 00:52:45,603 - load_generator - INFO - STARTING LOAD TEST: 5000 requests @ 8 concurrency\n",
      "2026-01-15 00:52:45,603 - load_generator - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOAD TEST RESULTS (Concurrency 4)\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Total Requests:             8\n",
      "  Successful:                 8\n",
      "  Failed:                     0\n",
      "  Success Rate:          100.00%\n",
      "  Total Duration:         26.16 seconds\n",
      "  Throughput:               0.3 tokens/sec\n",
      "  SLO Compliance:          0.00%\n",
      "  SLO Violations:             8\n",
      "  Escalation Rate:         0.00%\n",
      "\n",
      "TTFT (Time-to-First-Token) in milliseconds:\n",
      "  P50:  6644.73 ms\n",
      "  P75:  6733.25 ms\n",
      "  P90:  6903.55 ms\n",
      "  P95:  6928.60 ms\n",
      "  P99:  6948.65 ms\n",
      "  Mean: 6645.35 ms (±215.09)\n",
      "\n",
      "TPOT (Time-Per-Output-Token) in milliseconds:\n",
      "  P50:  6160.02 ms\n",
      "  P75:  7010.10 ms\n",
      "  P90:  7105.19 ms\n",
      "  P95:  7121.94 ms\n",
      "  P99:  7135.35 ms\n",
      "  Mean: 6277.51 ms (±681.63)\n",
      "\n",
      "E2E Latency (End-to-End) in milliseconds:\n",
      "  P50:  12702.44 ms\n",
      "  P75:  13640.99 ms\n",
      "  P90:  13792.09 ms\n",
      "  P95:  13808.04 ms\n",
      "  P99:  13820.80 ms\n",
      "  Mean: 12927.30 ms (±649.90)\n",
      "\n",
      "Queue Wait Time in milliseconds:\n",
      "  P50:     0.00 ms\n",
      "  P95:     0.00 ms\n",
      "  P99:     0.00 ms\n",
      "  Mean:    0.00 ms\n",
      "\n",
      "Sample SLO Violations (first 10):\n",
      "  1. Request 1 (easy)\n",
      "     TTFT: 6191.3ms > 200ms SLO\n",
      "     TPOT: 6476.2ms > 20ms SLO\n",
      "  2. Request 3 (easy)\n",
      "     TTFT: 6609.8ms > 200ms SLO\n",
      "     TPOT: 6983.2ms > 20ms SLO\n",
      "  3. Request 0 (easy)\n",
      "     TTFT: 6683.6ms > 200ms SLO\n",
      "     TPOT: 7090.8ms > 20ms SLO\n",
      "  4. Request 2 (easy)\n",
      "     TTFT: 6679.7ms > 200ms SLO\n",
      "     TPOT: 7138.7ms > 20ms SLO\n",
      "  5. Request 4 (easy)\n",
      "     TTFT: 6882.1ms > 200ms SLO\n",
      "     TPOT: 5843.8ms > 20ms SLO\n",
      "  6. Request 5 (easy)\n",
      "     TTFT: 6600.8ms > 200ms SLO\n",
      "     TPOT: 5692.4ms > 20ms SLO\n",
      "  7. Request 6 (easy)\n",
      "     TTFT: 6561.9ms > 200ms SLO\n",
      "     TPOT: 5617.9ms > 20ms SLO\n",
      "  8. Request 7 (easy)\n",
      "     TTFT: 6953.7ms > 200ms SLO\n",
      "     TPOT: 5377.1ms > 20ms SLO\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 00:53:42,967 - load_generator - INFO - Load test complete in 57.4s\n",
      "2026-01-15 00:53:42,968 - metrics - INFO - Initialized MetricsCalculator with 16 metrics\n",
      "2026-01-15 00:53:42,971 - metrics - INFO - Saved metrics to ./data/processed/metrics_concurrency_8.json\n",
      "2026-01-15 00:53:42,972 - load_generator - INFO - Saved metrics to ./data/processed/requests_concurrency_8.jsonl\n",
      "2026-01-15 00:53:42,973 - __main__ - INFO - \n",
      ">>> Testing with concurrency=16\n",
      "2026-01-15 00:53:42,973 - load_generator - INFO - Initialized ClosedLoopLoadGenerator\n",
      "2026-01-15 00:53:42,973 - load_generator - INFO -   Concurrency: 16\n",
      "2026-01-15 00:53:42,973 - load_generator - INFO -   Total requests: 5000\n",
      "2026-01-15 00:53:42,974 - load_generator - INFO -   Data pool size: 4873\n",
      "2026-01-15 00:53:42,974 - load_generator - INFO - ======================================================================\n",
      "2026-01-15 00:53:42,974 - load_generator - INFO - STARTING LOAD TEST: 5000 requests @ 16 concurrency\n",
      "2026-01-15 00:53:42,974 - load_generator - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOAD TEST RESULTS (Concurrency 8)\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "  Total Requests:            16\n",
      "  Successful:                16\n",
      "  Failed:                     0\n",
      "  Success Rate:          100.00%\n",
      "  Total Duration:         57.36 seconds\n",
      "  Throughput:               0.3 tokens/sec\n",
      "  SLO Compliance:          0.00%\n",
      "  SLO Violations:            16\n",
      "  Escalation Rate:         0.00%\n",
      "\n",
      "TTFT (Time-to-First-Token) in milliseconds:\n",
      "  P50:  14364.14 ms\n",
      "  P75:  15574.28 ms\n",
      "  P90:  16601.48 ms\n",
      "  P95:  17008.76 ms\n",
      "  P99:  17124.68 ms\n",
      "  Mean: 14706.20 ms (±1293.61)\n",
      "\n",
      "TPOT (Time-Per-Output-Token) in milliseconds:\n",
      "  P50:  13347.26 ms\n",
      "  P75:  15820.80 ms\n",
      "  P90:  17352.11 ms\n",
      "  P95:  17437.44 ms\n",
      "  P99:  17597.75 ms\n",
      "  Mean: 13707.72 ms (±2705.27)\n",
      "\n",
      "E2E Latency (End-to-End) in milliseconds:\n",
      "  P50:  28444.74 ms\n",
      "  P75:  29468.61 ms\n",
      "  P90:  31493.13 ms\n",
      "  P95:  31576.07 ms\n",
      "  P99:  31760.81 ms\n",
      "  Mean: 28420.82 ms (±2088.88)\n",
      "\n",
      "Queue Wait Time in milliseconds:\n",
      "  P50:     0.00 ms\n",
      "  P95:     0.01 ms\n",
      "  P99:     0.01 ms\n",
      "  Mean:    0.00 ms\n",
      "\n",
      "Sample SLO Violations (first 10):\n",
      "  1. Request 4 (easy)\n",
      "     TTFT: 13011.6ms > 200ms SLO\n",
      "     TPOT: 14935.8ms > 20ms SLO\n",
      "  2. Request 5 (easy)\n",
      "     TTFT: 13170.0ms > 200ms SLO\n",
      "     TPOT: 15255.4ms > 20ms SLO\n",
      "  3. Request 1 (easy)\n",
      "     TTFT: 13168.6ms > 200ms SLO\n",
      "     TPOT: 15275.0ms > 20ms SLO\n",
      "  4. Request 6 (easy)\n",
      "     TTFT: 13265.0ms > 200ms SLO\n",
      "     TPOT: 15410.7ms > 20ms SLO\n",
      "  5. Request 3 (easy)\n",
      "     TTFT: 14061.6ms > 200ms SLO\n",
      "     TPOT: 17051.2ms > 20ms SLO\n",
      "  6. Request 2 (easy)\n",
      "     TTFT: 14112.9ms > 200ms SLO\n",
      "     TPOT: 17370.6ms > 20ms SLO\n",
      "  7. Request 0 (easy)\n",
      "     TTFT: 14159.9ms > 200ms SLO\n",
      "     TPOT: 17333.6ms > 20ms SLO\n",
      "  8. Request 7 (easy)\n",
      "     TTFT: 14166.0ms > 200ms SLO\n",
      "     TPOT: 17637.8ms > 20ms SLO\n",
      "  9. Request 11 (easy)\n",
      "     TTFT: 15949.1ms > 200ms SLO\n",
      "     TPOT: 11316.4ms > 20ms SLO\n",
      "  10. Request 10 (easy)\n",
      "     TTFT: 16242.5ms > 200ms SLO\n",
      "     TPOT: 11449.5ms > 20ms SLO\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "results = run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"End-to-end baseline MED-only server evaluation\"\n",
    "#     )\n",
    "    \n",
    "#     # Data configuration\n",
    "#     parser.add_argument(\"--preprocess\", action=\"store_true\",\n",
    "#                        help=\"Run preprocessing (download and process datasets)\")\n",
    "#     parser.add_argument(\"--data_dir\", default=\"data/raw\",\n",
    "#                        help=\"Raw data directory\")\n",
    "#     parser.add_argument(\"--processed_dir\", default=\"data/processed\",\n",
    "#                        help=\"Processed data directory\")\n",
    "#     parser.add_argument(\"--data_subset\", type=int, default=0,\n",
    "#                        help=\"Use subset of data (0=all, >0=limit to N examples)\")\n",
    "    \n",
    "#     # Model configuration\n",
    "#     parser.add_argument(\"--model_name\", \n",
    "#                        default=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                        help=\"HuggingFace model name\")\n",
    "#     parser.add_argument(\"--device\", default=\"cuda\",\n",
    "#                        help=\"Device: 'cuda' or 'cpu'\")\n",
    "#     parser.add_argument(\"--dtype\", default=\"auto\",\n",
    "#                        help=\"Data type: 'auto', 'float16', 'bfloat16'\")\n",
    "    \n",
    "#     # Load test configuration\n",
    "#     parser.add_argument(\"--num_requests\", type=int, default=5000,\n",
    "#                        help=\"Number of requests per concurrency level\")\n",
    "#     parser.add_argument(\"--concurrencies\", type=int, nargs=\"+\",\n",
    "#                        default=[1, 2, 4, 8, 16, 32],\n",
    "#                        help=\"Concurrency levels to test\")\n",
    "    \n",
    "#     # Output configuration\n",
    "#     parser.add_argument(\"--output_dir\", default=\"results/baseline_med\",\n",
    "#                        help=\"Output directory for results\")\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     logger.info(f\"Configuration:\")\n",
    "#     logger.info(f\"  Model: {args.model_name}\")\n",
    "#     logger.info(f\"  Device: {args.device}\")\n",
    "#     logger.info(f\"  Requests per test: {args.num_requests}\")\n",
    "#     logger.info(f\"  Concurrency levels: {args.concurrencies}\")\n",
    "#     logger.info(f\"  Output dir: {args.output_dir}\")\n",
    "    \n",
    "#     main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
