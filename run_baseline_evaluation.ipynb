{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ccaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_baseline_evaluation.py\n",
    "\"\"\"\n",
    "End-to-end baseline evaluation orchestration\n",
    "Runs complete pipeline: preprocessing → server → load tests → evaluation\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from preprocessing import DataPreprocessor\n",
    "from server import SingleVariantServer\n",
    "from load_generator import ClosedLoopLoadGenerator\n",
    "from metrics import MetricsCalculator\n",
    "from evaluation import HeldOutEvaluator\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_data(data_dir: str = \"data/processed\"):\n",
    "    \"\"\"Load preprocessed datasets from JSONL files\"\"\"\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for split_name, split_list in [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]:\n",
    "        path = os.path.join(data_dir, f\"{split_name}_data.jsonl\")\n",
    "        if os.path.exists(path):\n",
    "            logger.info(f\"Loading {split_name} data from {path}\")\n",
    "            with open(path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        split_list.append(json.loads(line))\n",
    "        else:\n",
    "            logger.warning(f\"File not found: {path}\")\n",
    "    \n",
    "    logger.info(f\"Loaded data: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c110277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"Run end-to-end baseline evaluation pipeline\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"END-TO-END BASELINE EVALUATION: MED-ONLY SERVER (8-BIT QUANTIZATION)\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Data preprocessing (if needed)\n",
    "    if args.preprocess:\n",
    "        logger.info(\"\\n[STEP 0] PREPROCESSING DATASETS\")\n",
    "        logger.info(\"-\"*80)\n",
    "        \n",
    "        preprocessor = DataPreprocessor(\n",
    "            data_dir=args.data_dir,\n",
    "            output_dir=args.processed_dir\n",
    "        )\n",
    "        train, val, test = preprocessor.run_pipeline()\n",
    "    \n",
    "    # Step 2: Load preprocessed data\n",
    "    logger.info(\"\\n[STEP 1] LOADING DATA\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    train_data, val_data, test_data = load_data(args.processed_dir)\n",
    "    \n",
    "    if not val_data or not test_data:\n",
    "        logger.error(\"No validation or test data found!\")\n",
    "        return\n",
    "    \n",
    "    # Use subset for faster iteration (optional)\n",
    "    if args.data_subset > 0:\n",
    "        val_data = val_data[:args.data_subset]\n",
    "        test_data = test_data[:args.data_subset]\n",
    "        logger.info(f\"Using subset: val={len(val_data)}, test={len(test_data)}\")\n",
    "    \n",
    "    # Step 3: Initialize server\n",
    "    logger.info(\"\\n[STEP 2] INITIALIZING SERVER\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        server = SingleVariantServer(\n",
    "            model_name=args.model_name,\n",
    "            variant=\"med\",\n",
    "            device=args.device,\n",
    "            dtype=args.dtype\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize server: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Run load tests at multiple concurrency levels\n",
    "    logger.info(\"\\n[STEP 3] RUNNING LOAD TESTS\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    load_test_results = {}\n",
    "    all_metrics_summary = []\n",
    "    \n",
    "    for concurrency in args.concurrencies:\n",
    "        logger.info(f\"\\n>>> Testing with concurrency={concurrency}\")\n",
    "        \n",
    "        # Create load generator\n",
    "        load_gen = ClosedLoopLoadGenerator(\n",
    "            inference_func=server.generate,\n",
    "            max_concurrency=concurrency,\n",
    "            num_requests=args.num_requests,\n",
    "            data_loader=val_data\n",
    "        )\n",
    "        \n",
    "        # Run load test\n",
    "        start_time = time.time()\n",
    "        metrics = load_gen.run()\n",
    "        load_duration = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        calc = MetricsCalculator(metrics)\n",
    "        test_metrics = calc.compute_all_metrics()\n",
    "        load_test_results[concurrency] = test_metrics\n",
    "        \n",
    "        # Print report\n",
    "        calc.print_report(\n",
    "            title=f\"LOAD TEST RESULTS (Concurrency {concurrency})\"\n",
    "        )\n",
    "        \n",
    "        # Save detailed metrics to JSON\n",
    "        metrics_file = os.path.join(\n",
    "            args.output_dir,\n",
    "            f\"metrics_concurrency_{concurrency}.json\"\n",
    "        )\n",
    "        calc.save_metrics(metrics_file)\n",
    "        \n",
    "        # Save individual request logs\n",
    "        requests_file = os.path.join(\n",
    "            args.output_dir,\n",
    "            f\"requests_concurrency_{concurrency}.jsonl\"\n",
    "        )\n",
    "        load_gen.save_metrics(requests_file)\n",
    "        \n",
    "        # Summary entry\n",
    "        summary_entry = {\n",
    "            \"concurrency\": concurrency,\n",
    "            \"num_requests\": args.num_requests,\n",
    "            \"duration_sec\": load_duration,\n",
    "            \"success_rate\": test_metrics[\"summary\"][\"success_rate\"],\n",
    "            \"throughput_tokens_per_sec\": test_metrics[\"summary\"][\"throughput_tokens_per_sec\"],\n",
    "            \"ttft_p99_ms\": test_metrics[\"ttft\"][\"p99\"],\n",
    "            \"tpot_p95_ms\": test_metrics[\"tpot\"][\"p95\"],\n",
    "            \"e2e_p99_ms\": test_metrics[\"e2e_latency\"][\"p99\"],\n",
    "            \"slo_compliance\": test_metrics[\"summary\"][\"slo_compliance\"],\n",
    "            \"slo_violations\": test_metrics[\"summary\"][\"slo_violations\"]\n",
    "        }\n",
    "        all_metrics_summary.append(summary_entry)\n",
    "    \n",
    "    # Step 5: Evaluate accuracy on held-out test set\n",
    "    logger.info(\"\\n[STEP 4] EVALUATING ACCURACY\")\n",
    "    logger.info(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        evaluator = HeldOutEvaluator(\n",
    "            model=server,\n",
    "            data_loader=test_data,\n",
    "            batch_size=32\n",
    "        )\n",
    "        eval_results = evaluator.evaluate()\n",
    "        \n",
    "        # Save eval results\n",
    "        eval_file = os.path.join(args.output_dir, \"eval_results.json\")\n",
    "        with open(eval_file, 'w') as f:\n",
    "            json.dump(eval_results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved evaluation results to {eval_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")\n",
    "        eval_results = {}\n",
    "    \n",
    "    # Step 6: Summary report\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"FINAL SUMMARY REPORT\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nLoad Test Results by Concurrency:\")\n",
    "    print(f\"{'Concurrency':<12} {'Throughput':<18} {'TTFT P99':<12} {'TPOT P95':<12} {'E2E P99':<12} {'SLO Compl':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for summary in all_metrics_summary:\n",
    "        print(f\"{summary['concurrency']:<12} \"\n",
    "              f\"{summary['throughput_tokens_per_sec']:<18.1f} \"\n",
    "              f\"{summary['ttft_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['tpot_p95_ms']:<12.1f} \"\n",
    "              f\"{summary['e2e_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['slo_compliance']*100:<10.1f}%\")\n",
    "    \n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    if eval_results:\n",
    "        for dataset_type in sorted(eval_results.keys()):\n",
    "            if dataset_type != \"overall\":\n",
    "                result = eval_results[dataset_type]\n",
    "                print(f\"  {dataset_type.upper():<10s}: {result['em']*100:6.2f}% \"\n",
    "                      f\"({result['correct_count']}/{result['total_count']})\")\n",
    "        \n",
    "        overall = eval_results.get(\"overall\", {})\n",
    "        print(f\"  {'OVERALL':<10s}: {overall.get('em', 0)*100:6.2f}% \"\n",
    "              f\"({overall.get('correct_count', 0)}/{overall.get('total_count', 0)})\")\n",
    "    else:\n",
    "        print(\"  No evaluation results available\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = os.path.join(args.output_dir, \"summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"load_test_summary\": all_metrics_summary,\n",
    "            \"eval_results\": eval_results,\n",
    "            \"config\": {\n",
    "                \"model_name\": args.model_name,\n",
    "                \"variant\": \"med\",\n",
    "                \"num_requests\": args.num_requests,\n",
    "                \"concurrencies\": args.concurrencies,\n",
    "                \"device\": args.device\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"BASELINE EVALUATION COMPLETE\")\n",
    "    logger.info(f\"Results saved to: {args.output_dir}\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"\\nFiles generated:\")\n",
    "    logger.info(f\"  - summary.json (overall summary)\")\n",
    "    logger.info(f\"  - eval_results.json (accuracy metrics)\")\n",
    "    for concurrency in args.concurrencies:\n",
    "        logger.info(f\"  - metrics_concurrency_{concurrency}.json\")\n",
    "        logger.info(f\"  - requests_concurrency_{concurrency}.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bba44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Define your config here (edit directly in the notebook)\n",
    "config = {\n",
    "    \"output_dir\": \"./data/processed\",\n",
    "    \"data_dir\": \"./results/baseline_med\",\n",
    "    \"processed_dir\": \"./data/processed\",\n",
    "    \"preprocess\": True,\n",
    "    \"data_subset\": 0,                 # 0 = use all; >0 = use first N\n",
    "    \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"dtype\": \"float16\",\n",
    "    \"concurrencies\": [1,2,4,8,16,32],\n",
    "    \"num_requests\": 5000,\n",
    "}\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Run end-to-end baseline evaluation pipeline in notebook.\"\"\"\n",
    "\n",
    "    # Convenience local vars (to avoid args.)\n",
    "    output_dir     = config[\"output_dir\"]\n",
    "    data_dir       = config[\"data_dir\"]\n",
    "    processed_dir  = config[\"processed_dir\"]\n",
    "    preprocess     = config[\"preprocess\"]\n",
    "    data_subset    = config[\"data_subset\"]\n",
    "    model_name     = config[\"model_name\"]\n",
    "    device         = config[\"device\"]\n",
    "    dtype          = config[\"dtype\"]\n",
    "    concurrencies  = config[\"concurrencies\"]\n",
    "    num_requests   = config[\"num_requests\"]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"END-TO-END BASELINE EVALUATION: MED-ONLY SERVER (8-BIT QUANTIZATION)\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    # Step 1: Data preprocessing (if needed)\n",
    "    if preprocess:\n",
    "        logger.info(\"\\n[STEP 0] PREPROCESSING DATASETS\")\n",
    "        logger.info(\"-\"*80)\n",
    "\n",
    "        preprocessor = DataPreprocessor(\n",
    "            data_dir=data_dir,\n",
    "            output_dir=processed_dir\n",
    "        )\n",
    "        train, val, test = preprocessor.run_pipeline()\n",
    "\n",
    "    # Step 2: Load preprocessed data\n",
    "    logger.info(\"\\n[STEP 1] LOADING DATA\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    train_data, val_data, test_data = load_data(processed_dir)\n",
    "\n",
    "    if not val_data or not test_data:\n",
    "        logger.error(\"No validation or test data found!\")\n",
    "        return\n",
    "\n",
    "    # Use subset for faster iteration (optional)\n",
    "    if data_subset > 0:\n",
    "        val_data = val_data[:data_subset]\n",
    "        test_data = test_data[:data_subset]\n",
    "        logger.info(f\"Using subset: val={len(val_data)}, test={len(test_data)}\")\n",
    "\n",
    "    # Step 3: Initialize server\n",
    "    logger.info(\"\\n[STEP 2] INITIALIZING SERVER\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    try:\n",
    "        server = SingleVariantServer(\n",
    "            model_name=model_name,\n",
    "            variant=\"med\",\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize server: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 4: Run load tests at multiple concurrency levels\n",
    "    logger.info(\"\\n[STEP 3] RUNNING LOAD TESTS\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    load_test_results = {}\n",
    "    all_metrics_summary = []\n",
    "\n",
    "    for concurrency in concurrencies:\n",
    "        logger.info(f\"\\n>>> Testing with concurrency={concurrency}\")\n",
    "\n",
    "        # Create load generator\n",
    "        load_gen = ClosedLoopLoadGenerator(\n",
    "            inference_func=server.generate,\n",
    "            max_concurrency=concurrency,\n",
    "            num_requests=num_requests,\n",
    "            data_loader=val_data\n",
    "        )\n",
    "\n",
    "        # Run load test\n",
    "        start_time = time.time()\n",
    "        metrics = load_gen.run()\n",
    "        load_duration = time.time() - start_time\n",
    "\n",
    "        # Calculate metrics\n",
    "        calc = MetricsCalculator(metrics)\n",
    "        test_metrics = calc.compute_all_metrics()\n",
    "        load_test_results[concurrency] = test_metrics\n",
    "\n",
    "        # Print report\n",
    "        calc.print_report(\n",
    "            title=f\"LOAD TEST RESULTS (Concurrency {concurrency})\"\n",
    "        )\n",
    "\n",
    "        # Save detailed metrics to JSON\n",
    "        metrics_file = os.path.join(\n",
    "            output_dir,\n",
    "            f\"metrics_concurrency_{concurrency}.json\"\n",
    "        )\n",
    "        calc.save_metrics(metrics_file)\n",
    "\n",
    "        # Save individual request logs\n",
    "        requests_file = os.path.join(\n",
    "            output_dir,\n",
    "            f\"requests_concurrency_{concurrency}.jsonl\"\n",
    "        )\n",
    "        load_gen.save_metrics(requests_file)\n",
    "\n",
    "        # Summary entry\n",
    "        summary_entry = {\n",
    "            \"concurrency\": concurrency,\n",
    "            \"num_requests\": num_requests,\n",
    "            \"duration_sec\": load_duration,\n",
    "            \"success_rate\": test_metrics[\"summary\"][\"success_rate\"],\n",
    "            \"throughput_tokens_per_sec\": test_metrics[\"summary\"][\"throughput_tokens_per_sec\"],\n",
    "            \"ttft_p99_ms\": test_metrics[\"ttft\"][\"p99\"],\n",
    "            \"tpot_p95_ms\": test_metrics[\"tpot\"][\"p95\"],\n",
    "            \"e2e_p99_ms\": test_metrics[\"e2e_latency\"][\"p99\"],\n",
    "            \"slo_compliance\": test_metrics[\"summary\"][\"slo_compliance\"],\n",
    "            \"slo_violations\": test_metrics[\"summary\"][\"slo_violations\"]\n",
    "        }\n",
    "        all_metrics_summary.append(summary_entry)\n",
    "\n",
    "    # Step 5: Evaluate accuracy on held-out test set\n",
    "    logger.info(\"\\n[STEP 4] EVALUATING ACCURACY\")\n",
    "    logger.info(\"-\"*80)\n",
    "\n",
    "    try:\n",
    "        evaluator = HeldOutEvaluator(\n",
    "            model=server,\n",
    "            data_loader=test_data,\n",
    "            batch_size=32\n",
    "        )\n",
    "        eval_results = evaluator.evaluate()\n",
    "\n",
    "        # Save eval results\n",
    "        eval_file = os.path.join(output_dir, \"eval_results.json\")\n",
    "        with open(eval_file, 'w') as f:\n",
    "            json.dump(eval_results, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Saved evaluation results to {eval_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")\n",
    "        eval_results = {}\n",
    "\n",
    "    # Step 6: Summary report\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"FINAL SUMMARY REPORT\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    print(\"\\nLoad Test Results by Concurrency:\")\n",
    "    print(f\"{'Concurrency':<12} {'Throughput':<18} {'TTFT P99':<12} {'TPOT P95':<12} {'E2E P99':<12} {'SLO Compl':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for summary in all_metrics_summary:\n",
    "        print(f\"{summary['concurrency']:<12} \"\n",
    "              f\"{summary['throughput_tokens_per_sec']:<18.1f} \"\n",
    "              f\"{summary['ttft_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['tpot_p95_ms']:<12.1f} \"\n",
    "              f\"{summary['e2e_p99_ms']:<12.1f} \"\n",
    "              f\"{summary['slo_compliance']*100:<10.1f}%\")\n",
    "\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    if eval_results:\n",
    "        for dataset_type in sorted(eval_results.keys()):\n",
    "            if dataset_type != \"overall\":\n",
    "                result = eval_results[dataset_type]\n",
    "                print(f\"  {dataset_type.upper():<10s}: {result['em']*100:6.2f}% \"\n",
    "                      f\"({result['correct_count']}/{result['total_count']})\")\n",
    "\n",
    "        overall = eval_results.get(\"overall\", {})\n",
    "        print(f\"  {'OVERALL':<10s}: {overall.get('em', 0)*100:6.2f}% \"\n",
    "              f\"({overall.get('correct_count', 0)}/{overall.get('total_count', 0)})\")\n",
    "    else:\n",
    "        print(\"  No evaluation results available\")\n",
    "\n",
    "    # Save summary\n",
    "    summary_file = os.path.join(output_dir, \"summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"load_test_summary\": all_metrics_summary,\n",
    "            \"eval_results\": eval_results,\n",
    "            \"config\": {\n",
    "                \"model_name\": model_name,\n",
    "                \"variant\": \"med\",\n",
    "                \"num_requests\": num_requests,\n",
    "                \"concurrencies\": concurrencies,\n",
    "                \"device\": device\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"BASELINE EVALUATION COMPLETE\")\n",
    "    logger.info(f\"Results saved to: {output_dir}\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"\\nFiles generated:\")\n",
    "    logger.info(f\"  - summary.json (overall summary)\")\n",
    "    logger.info(f\"  - eval_results.json (accuracy metrics)\")\n",
    "    for concurrency in concurrencies:\n",
    "        logger.info(f\"  - metrics_concurrency_{concurrency}.json\")\n",
    "        logger.info(f\"  - requests_concurrency_{concurrency}.jsonl\")\n",
    "\n",
    "    # Optionally return things for further notebook analysis\n",
    "    return {\n",
    "        \"load_test_summary\": all_metrics_summary,\n",
    "        \"eval_results\": eval_results,\n",
    "        \"config\": config,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9be8b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 00:21:09,568 - __main__ - INFO - ================================================================================\n",
      "2026-01-14 00:21:09,569 - __main__ - INFO - END-TO-END BASELINE EVALUATION: MED-ONLY SERVER (8-BIT QUANTIZATION)\n",
      "2026-01-14 00:21:09,569 - __main__ - INFO - ================================================================================\n",
      "2026-01-14 00:21:09,570 - __main__ - INFO - \n",
      "[STEP 0] PREPROCESSING DATASETS\n",
      "2026-01-14 00:21:09,570 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-14 00:21:09,571 - preprocessing - INFO - Initialized DataPreprocessor\n",
      "2026-01-14 00:21:09,571 - preprocessing - INFO -   Data dir: ./results/baseline_med\n",
      "2026-01-14 00:21:09,572 - preprocessing - INFO -   Output dir: ./data/processed\n",
      "2026-01-14 00:21:09,572 - preprocessing - INFO - ======================================================================\n",
      "2026-01-14 00:21:09,572 - preprocessing - INFO - STARTING DATA PREPROCESSING PIPELINE\n",
      "2026-01-14 00:21:09,572 - preprocessing - INFO - ======================================================================\n",
      "2026-01-14 00:21:09,573 - preprocessing - INFO - \n",
      "[STEP 1] Processing MMLU\n",
      "2026-01-14 00:21:09,573 - preprocessing - INFO - ----------------------------------------------------------------------\n",
      "2026-01-14 00:21:09,573 - preprocessing - INFO - Processing MMLU dataset...\n",
      "2026-01-14 00:21:11,551 - preprocessing - INFO -   Processing MMLU validation split: 1531 examples\n",
      "2026-01-14 00:21:11,608 - preprocessing - INFO -   Processing MMLU test split: 14042 examples\n",
      "2026-01-14 00:21:12,154 - preprocessing - INFO - Processed 15573 MMLU examples\n",
      "2026-01-14 00:21:12,200 - preprocessing - INFO - Saved MMLU to ./data/processed/mmlu_processed.jsonl\n",
      "2026-01-14 00:21:12,201 - preprocessing - INFO - \n",
      "[STEP 2] Processing GSM8K\n",
      "2026-01-14 00:21:12,202 - preprocessing - INFO - ----------------------------------------------------------------------\n",
      "2026-01-14 00:21:12,202 - preprocessing - INFO - Processing GSM8K dataset...\n",
      "2026-01-14 00:21:13,686 - preprocessing - INFO -   Processing GSM8K train split: 7473 examples\n",
      "2026-01-14 00:21:13,921 - preprocessing - INFO -   Processing GSM8K test split: 1319 examples\n",
      "2026-01-14 00:21:13,960 - preprocessing - INFO - Processed 8792 GSM8K examples\n",
      "2026-01-14 00:21:13,981 - preprocessing - INFO - Saved GSM8K to ./data/processed/gsm8k_processed.jsonl\n",
      "2026-01-14 00:21:13,981 - preprocessing - INFO - \n",
      "[STEP 3] Combining and creating splits\n",
      "2026-01-14 00:21:13,981 - preprocessing - INFO - ----------------------------------------------------------------------\n",
      "2026-01-14 00:21:13,982 - preprocessing - INFO - Combining datasets and creating splits...\n",
      "2026-01-14 00:21:13,982 - preprocessing - INFO - Loading mmlu_processed...\n",
      "2026-01-14 00:21:14,024 - preprocessing - INFO - Loading gsm8k_processed...\n",
      "2026-01-14 00:21:14,043 - preprocessing - INFO - Total examples loaded: 24365\n",
      "2026-01-14 00:21:14,045 - preprocessing - INFO -   easy  : 4093 examples\n",
      "2026-01-14 00:21:14,047 - preprocessing - INFO -   medium: 16480 examples\n",
      "2026-01-14 00:21:14,049 - preprocessing - INFO -   hard  : 3792 examples\n",
      "2026-01-14 00:21:14,098 - preprocessing - INFO - train split: 14618 examples | Easy:  2455 | Medium:  9888 | Hard:  2275\n",
      "2026-01-14 00:21:14,115 - preprocessing - INFO - val   split:  4873 examples | Easy:   819 | Medium:  3296 | Hard:   758\n",
      "2026-01-14 00:21:14,133 - preprocessing - INFO - test  split:  4874 examples | Easy:   819 | Medium:  3296 | Hard:   759\n",
      "2026-01-14 00:21:14,133 - preprocessing - INFO - \n",
      "======================================================================\n",
      "2026-01-14 00:21:14,133 - preprocessing - INFO - PREPROCESSING COMPLETE\n",
      "2026-01-14 00:21:14,133 - preprocessing - INFO - ======================================================================\n",
      "2026-01-14 00:21:14,137 - __main__ - INFO - \n",
      "[STEP 1] LOADING DATA\n",
      "2026-01-14 00:21:14,137 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-14 00:21:14,137 - __main__ - INFO - Loading train data from ./data/processed/train_data.jsonl\n",
      "2026-01-14 00:21:14,178 - __main__ - INFO - Loading val data from ./data/processed/val_data.jsonl\n",
      "2026-01-14 00:21:14,192 - __main__ - INFO - Loading test data from ./data/processed/test_data.jsonl\n",
      "2026-01-14 00:21:14,206 - __main__ - INFO - Loaded data: train=14618, val=4873, test=4874\n",
      "2026-01-14 00:21:14,206 - __main__ - INFO - \n",
      "[STEP 2] INITIALIZING SERVER\n",
      "2026-01-14 00:21:14,206 - __main__ - INFO - --------------------------------------------------------------------------------\n",
      "2026-01-14 00:21:14,207 - server - INFO - Initializing MED server\n",
      "2026-01-14 00:21:14,207 - server - INFO -   Model: meta-llama/Llama-2-7b-chat-hf\n",
      "2026-01-14 00:21:14,207 - server - INFO -   Device: cuda\n",
      "2026-01-14 00:21:14,207 - server - INFO -   Dtype: float16\n",
      "2026-01-14 00:21:14,678 - server - ERROR - Failed to load tokenizer: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
      "403 Client Error. (Request ID: Root=1-69669422-345c22786055988451f07a34;70dd1b7d-907c-4b7a-a2a6-9e9b9a68b559)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors.\n",
      "2026-01-14 00:21:14,679 - __main__ - ERROR - Failed to initialize server: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
      "403 Client Error. (Request ID: Root=1-69669422-345c22786055988451f07a34;70dd1b7d-907c-4b7a-a2a6-9e9b9a68b559)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors.\n"
     ]
    }
   ],
   "source": [
    "results = run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"End-to-end baseline MED-only server evaluation\"\n",
    "#     )\n",
    "    \n",
    "#     # Data configuration\n",
    "#     parser.add_argument(\"--preprocess\", action=\"store_true\",\n",
    "#                        help=\"Run preprocessing (download and process datasets)\")\n",
    "#     parser.add_argument(\"--data_dir\", default=\"data/raw\",\n",
    "#                        help=\"Raw data directory\")\n",
    "#     parser.add_argument(\"--processed_dir\", default=\"data/processed\",\n",
    "#                        help=\"Processed data directory\")\n",
    "#     parser.add_argument(\"--data_subset\", type=int, default=0,\n",
    "#                        help=\"Use subset of data (0=all, >0=limit to N examples)\")\n",
    "    \n",
    "#     # Model configuration\n",
    "#     parser.add_argument(\"--model_name\", \n",
    "#                        default=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                        help=\"HuggingFace model name\")\n",
    "#     parser.add_argument(\"--device\", default=\"cuda\",\n",
    "#                        help=\"Device: 'cuda' or 'cpu'\")\n",
    "#     parser.add_argument(\"--dtype\", default=\"auto\",\n",
    "#                        help=\"Data type: 'auto', 'float16', 'bfloat16'\")\n",
    "    \n",
    "#     # Load test configuration\n",
    "#     parser.add_argument(\"--num_requests\", type=int, default=5000,\n",
    "#                        help=\"Number of requests per concurrency level\")\n",
    "#     parser.add_argument(\"--concurrencies\", type=int, nargs=\"+\",\n",
    "#                        default=[1, 2, 4, 8, 16, 32],\n",
    "#                        help=\"Concurrency levels to test\")\n",
    "    \n",
    "#     # Output configuration\n",
    "#     parser.add_argument(\"--output_dir\", default=\"results/baseline_med\",\n",
    "#                        help=\"Output directory for results\")\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     logger.info(f\"Configuration:\")\n",
    "#     logger.info(f\"  Model: {args.model_name}\")\n",
    "#     logger.info(f\"  Device: {args.device}\")\n",
    "#     logger.info(f\"  Requests per test: {args.num_requests}\")\n",
    "#     logger.info(f\"  Concurrency levels: {args.concurrencies}\")\n",
    "#     logger.info(f\"  Output dir: {args.output_dir}\")\n",
    "    \n",
    "#     main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
